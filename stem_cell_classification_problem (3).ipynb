{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Import necessary library***"
      ],
      "metadata": {
        "id": "DhMvFKrZrVta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOPKoHFRd9Lp"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Want to open the images through Open_CV library**\n",
        " **However, imageFolder from touchvision directly access high quality tiff images**"
      ],
      "metadata": {
        "id": "Bhl2KRptrcv6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-uI6CCYsVg9"
      },
      "outputs": [],
      "source": [
        "def opencv_loader(P):\n",
        "#read image using open CV\n",
        "    img=cv2.imread(P)\n",
        "#convert open CV color famework into machine learning famework RGB colour    \n",
        "    img=cv2.cvtColor(img,cv2.COLOR_BRG2RGB)\n",
        "    return np.array(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image transformation from pixel size 2592 * 1944 to 32 * 32 pixcel as well**    **as our array data into tensor**"
      ],
      "metadata": {
        "id": "yKMx0Alhr7QG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwwzomIPfGaL"
      },
      "outputs": [],
      "source": [
        "mean=[0.5,0.5,0.5]\n",
        "std=[0.5,0.5,0.5]\n",
        "train_transforms=transforms.Compose([ transforms.RandomHorizontalFlip(),\n",
        "                                   transforms.RandomRotation(10),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize(torch.Tensor(mean),torch.Tensor(std)),\n",
        "                                   transforms.Resize((32,32))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY8eepdIfOYT"
      },
      "outputs": [],
      "source": [
        "mean=[0.5,0.5,0.5]\n",
        "std=[0.5,0.5,0.5]\n",
        "test_transforms=transforms.Compose([\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize(torch.Tensor(mean),torch.Tensor(std)),\n",
        "                                    transforms.Resize((32,32)) ])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data into path**"
      ],
      "metadata": {
        "id": "3tuIGOoDtKhC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpdQTwHSswFx"
      },
      "outputs": [],
      "source": [
        "test_path='/content/drive/MyDrive/Test Stem Cell'\n",
        "train_path='/content/drive/MyDrive/Train Stem Cell'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imagefolder**"
      ],
      "metadata": {
        "id": "aNPxypYDtRgR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPlUQ2b3gZhn"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_datasets=torchvision.datasets.ImageFolder(train_path,transform=train_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok3niG6Vsp4Z"
      },
      "outputs": [],
      "source": [
        "test_datasets=torchvision.datasets.ImageFolder(test_path,transform=test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV3sSkFl9hwr",
        "outputId": "e3cc7d90-73fe-4d6c-ca5c-b0927cb14a7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(test_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTQSh0zkAchK",
        "outputId": "346858a2-c1b0-46c6-eb8c-9dfbb4e49943"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "340"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(train_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loader**"
      ],
      "metadata": {
        "id": "vlr2BZSGtVq7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8dalAhSwXnH"
      },
      "outputs": [],
      "source": [
        "train_loader=torch.utils.data.DataLoader(train_datasets,batch_size=8,shuffle=True)\n",
        "test_loader=torch.utils.data.DataLoader(test_datasets,batch_size=2,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1imSAtqAzrL",
        "outputId": "f7650e49-c95b-4661-994f-f48c95f86d1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[[[-1.0000, -1.0000, -1.0000,  ...,  0.0333,  0.0245, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1549,  0.1451, -1.0000],\n",
              "           [ 0.1843,  0.1637,  0.1941,  ...,  0.0745,  0.2529, -1.0000],\n",
              "           ...,\n",
              "           [-1.0000,  0.3725,  0.2020,  ...,  0.1990,  0.1549,  0.1873],\n",
              "           [-1.0000,  0.2608,  0.3804,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.1784,  0.3539,  ..., -1.0000, -1.0000, -1.0000]],\n",
              " \n",
              "          [[-1.0000, -1.0000, -1.0000,  ...,  0.0333,  0.0245, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1549,  0.1451, -1.0000],\n",
              "           [ 0.1843,  0.1637,  0.1941,  ...,  0.0745,  0.2529, -1.0000],\n",
              "           ...,\n",
              "           [-1.0000,  0.3725,  0.2020,  ...,  0.1990,  0.1549,  0.1873],\n",
              "           [-1.0000,  0.2608,  0.3804,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.1784,  0.3539,  ..., -1.0000, -1.0000, -1.0000]],\n",
              " \n",
              "          [[-1.0000, -1.0000, -1.0000,  ...,  0.0333,  0.0245, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1549,  0.1451, -1.0000],\n",
              "           [ 0.1843,  0.1637,  0.1941,  ...,  0.0745,  0.2529, -1.0000],\n",
              "           ...,\n",
              "           [-1.0000,  0.3725,  0.2020,  ...,  0.1990,  0.1549,  0.1873],\n",
              "           [-1.0000,  0.2608,  0.3804,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.1784,  0.3539,  ..., -1.0000, -1.0000, -1.0000]]],\n",
              " \n",
              " \n",
              "         [[[-1.0000, -0.1716, -0.1412,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.1206, -0.1304,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.1931, -0.0382,  ..., -0.1608, -0.0735, -0.0990],\n",
              "           ...,\n",
              "           [ 0.0569,  0.0216,  0.1225,  ...,  0.1275,  0.1127, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.2078,  0.1402, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1255, -0.0098, -1.0000]],\n",
              " \n",
              "          [[-1.0000, -0.1716, -0.1412,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.1206, -0.1304,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.1931, -0.0382,  ..., -0.1608, -0.0735, -0.0990],\n",
              "           ...,\n",
              "           [ 0.0569,  0.0216,  0.1225,  ...,  0.1275,  0.1127, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.2078,  0.1402, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1255, -0.0098, -1.0000]],\n",
              " \n",
              "          [[-1.0000, -0.1716, -0.1412,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.1206, -0.1304,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.1931, -0.0382,  ..., -0.1608, -0.0735, -0.0990],\n",
              "           ...,\n",
              "           [ 0.0569,  0.0216,  0.1225,  ...,  0.1275,  0.1127, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.2078,  0.1402, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1255, -0.0098, -1.0000]]],\n",
              " \n",
              " \n",
              "         [[[-1.0000,  0.3049,  0.4608,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.3873,  0.4843,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.4745,  0.4843,  ...,  0.2794,  0.0647,  0.2118],\n",
              "           ...,\n",
              "           [ 0.3686,  0.4578,  0.4657,  ...,  0.1471,  0.3853, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.3039,  0.2461, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.2000,  0.3186, -1.0000]],\n",
              " \n",
              "          [[-1.0000,  0.3049,  0.4608,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.3873,  0.4843,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.4745,  0.4843,  ...,  0.2794,  0.0647,  0.2118],\n",
              "           ...,\n",
              "           [ 0.3686,  0.4578,  0.4657,  ...,  0.1471,  0.3853, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.3039,  0.2461, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.2000,  0.3186, -1.0000]],\n",
              " \n",
              "          [[-1.0000,  0.3049,  0.4608,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.3873,  0.4843,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.4745,  0.4843,  ...,  0.2794,  0.0647,  0.2118],\n",
              "           ...,\n",
              "           [ 0.3686,  0.4578,  0.4657,  ...,  0.1471,  0.3853, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.3039,  0.2461, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.2000,  0.3186, -1.0000]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[-0.0363, -0.0559,  0.1333,  ...,  0.0941,  0.0853,  0.0225],\n",
              "           [-0.1127,  0.0245,  0.0422,  ...,  0.2137,  0.0157,  0.1137],\n",
              "           [ 0.0343, -0.0020,  0.0402,  ...,  0.1167,  0.2235,  0.0667],\n",
              "           ...,\n",
              "           [ 0.1304,  0.1931,  0.2284,  ...,  0.2314,  0.2078,  0.1755],\n",
              "           [ 0.1108,  0.2000,  0.2471,  ...,  0.2069,  0.0922,  0.2137],\n",
              "           [ 0.0510,  0.1275,  0.0980,  ...,  0.0941,  0.1176,  0.0824]],\n",
              " \n",
              "          [[-0.0363, -0.0559,  0.1333,  ...,  0.0941,  0.0853,  0.0225],\n",
              "           [-0.1127,  0.0245,  0.0422,  ...,  0.2137,  0.0157,  0.1137],\n",
              "           [ 0.0343, -0.0020,  0.0402,  ...,  0.1167,  0.2235,  0.0667],\n",
              "           ...,\n",
              "           [ 0.1304,  0.1931,  0.2284,  ...,  0.2314,  0.2078,  0.1755],\n",
              "           [ 0.1108,  0.2000,  0.2471,  ...,  0.2069,  0.0922,  0.2137],\n",
              "           [ 0.0510,  0.1275,  0.0980,  ...,  0.0941,  0.1176,  0.0824]],\n",
              " \n",
              "          [[-0.0363, -0.0559,  0.1333,  ...,  0.0941,  0.0853,  0.0225],\n",
              "           [-0.1127,  0.0245,  0.0422,  ...,  0.2137,  0.0157,  0.1137],\n",
              "           [ 0.0343, -0.0020,  0.0402,  ...,  0.1167,  0.2235,  0.0667],\n",
              "           ...,\n",
              "           [ 0.1304,  0.1931,  0.2284,  ...,  0.2314,  0.2078,  0.1755],\n",
              "           [ 0.1108,  0.2000,  0.2471,  ...,  0.2069,  0.0922,  0.2137],\n",
              "           [ 0.0510,  0.1275,  0.0980,  ...,  0.0941,  0.1176,  0.0824]]],\n",
              " \n",
              " \n",
              "         [[[-1.0000, -0.0902, -0.1049,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.1402, -0.0971,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.0275,  0.0873,  ...,  0.0275,  0.0510, -1.0000],\n",
              "           ...,\n",
              "           [-1.0000,  0.0127,  0.0892,  ...,  0.2039,  0.1539, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1569,  0.1686, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1255,  0.1069, -1.0000]],\n",
              " \n",
              "          [[-1.0000, -0.0902, -0.1049,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.1402, -0.0971,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.0275,  0.0873,  ...,  0.0275,  0.0510, -1.0000],\n",
              "           ...,\n",
              "           [-1.0000,  0.0127,  0.0892,  ...,  0.2039,  0.1539, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1569,  0.1686, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1255,  0.1069, -1.0000]],\n",
              " \n",
              "          [[-1.0000, -0.0902, -0.1049,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.1402, -0.0971,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000, -0.0275,  0.0873,  ...,  0.0275,  0.0510, -1.0000],\n",
              "           ...,\n",
              "           [-1.0000,  0.0127,  0.0892,  ...,  0.2039,  0.1539, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1569,  0.1686, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.1255,  0.1069, -1.0000]]],\n",
              " \n",
              " \n",
              "         [[[-1.0000,  0.0382,  0.6882,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.1078,  0.0088,  ...,  0.0882,  0.1618,  0.1078],\n",
              "           [ 0.0608,  0.0794,  0.2167,  ...,  0.1255,  0.1118,  0.1706],\n",
              "           ...,\n",
              "           [ 0.0147,  0.1108,  0.0971,  ...,  0.3794,  0.2441,  0.1343],\n",
              "           [ 0.0676,  0.1176,  0.1618,  ...,  0.2137, -0.2020, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.2098,  0.2549, -1.0000]],\n",
              " \n",
              "          [[-1.0000,  0.0382,  0.6882,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.1078,  0.0088,  ...,  0.0882,  0.1618,  0.1078],\n",
              "           [ 0.0608,  0.0794,  0.2167,  ...,  0.1255,  0.1118,  0.1706],\n",
              "           ...,\n",
              "           [ 0.0147,  0.1108,  0.0971,  ...,  0.3794,  0.2441,  0.1343],\n",
              "           [ 0.0676,  0.1176,  0.1618,  ...,  0.2137, -0.2020, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.2098,  0.2549, -1.0000]],\n",
              " \n",
              "          [[-1.0000,  0.0382,  0.6882,  ..., -1.0000, -1.0000, -1.0000],\n",
              "           [-1.0000,  0.1078,  0.0088,  ...,  0.0882,  0.1618,  0.1078],\n",
              "           [ 0.0608,  0.0794,  0.2167,  ...,  0.1255,  0.1118,  0.1706],\n",
              "           ...,\n",
              "           [ 0.0147,  0.1108,  0.0971,  ...,  0.3794,  0.2441,  0.1343],\n",
              "           [ 0.0676,  0.1176,  0.1618,  ...,  0.2137, -0.2020, -1.0000],\n",
              "           [-1.0000, -1.0000, -1.0000,  ...,  0.2098,  0.2549, -1.0000]]]]),\n",
              " tensor([6, 7, 1, 6, 5, 5, 2, 4])]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pplfMIOQxJNW"
      },
      "outputs": [],
      "source": [
        "def show_transformed_images(dataset):\n",
        "    loader=torch.utils.data.DataLoader(dataset,batch_size=4,shuffle=True)\n",
        "    batch=next(iter(loader))\n",
        "    images,labels=batch\n",
        "    grid=torchvision.utils.make_grid(images,nrows=3)\n",
        "    plt.figure(figsize=(11,11))\n",
        "    plt.imshow(np.transpose(grid,(1,2,0)))\n",
        "    print('labels:',labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image_show**"
      ],
      "metadata": {
        "id": "eZZOEumotdIs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "SXUBOERWyAzm",
        "outputId": "a8c0b73c-083e-493f-dafa-3180e76b31ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels: tensor([0, 5, 1, 7])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 792x792 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "show_transformed_images(train_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej1akDTCyWbh"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model_Archicture**"
      ],
      "metadata": {
        "id": "7fbFU6ovtjA_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTvEQ11s19Ot"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        self.conv1=nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
        "        self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.conv2=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1, padding=1)\n",
        "        self.fc1=nn.Linear(64*8*8,128)\n",
        "        self.fc2=nn.Linear(128,84)\n",
        "        self.fc3=nn.Linear(84,8)\n",
        "    def forward(self,x):\n",
        "        x=self.pool(F.relu(self.conv1(x)))\n",
        "        x=self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.flatten(1)\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=F.relu(self.fc2(x))\n",
        "        x=self.fc3(x)\n",
        "        #x = F.relu(nn.Linear(x.size(1),8))\n",
        "        #x = F.relu(nn.Linear(128, 84))\n",
        "        #x = F.relu(nn.Linear(84, 8))\n",
        "        return x\n",
        "    \n",
        "   \n",
        "model=Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmipj3IiqLs7",
        "outputId": "6f6c6a77-50bb-43b7-93ec-c84885354708"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "t = torch.randn(1,2,3,4)\n",
        "t = t.flatten(1)\n",
        "t.size(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnEFpGnP2AGk"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a Loss function and optimizer**"
      ],
      "metadata": {
        "id": "tNikQyVetpob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7h2nwMP2DZY"
      },
      "outputs": [],
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.001,momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhCH3wDL6IuU"
      },
      "outputs": [],
      "source": [
        "inputs, labels = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFBdXaG7q3Ur"
      },
      "outputs": [],
      "source": [
        "inputs, labels = next(iter(train_loader))\n",
        "optimizer.zero_grad()\n",
        "pred_output = model(inputs)\n",
        "loss=criterion(pred_output,labels)\n",
        "loss.backward()\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the network**"
      ],
      "metadata": {
        "id": "WLbJNb-5t2e0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA1Xl-zAHmec",
        "outputId": "1301447e-9bae-4416-f1cd-333b9b132fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   10] loss:2.086\n",
            "[1,   20] loss:2.071\n",
            "[1,   30] loss:2.081\n",
            "[1,   40] loss:2.083\n",
            "[2,   10] loss:2.084\n",
            "[2,   20] loss:2.073\n",
            "[2,   30] loss:2.075\n",
            "[2,   40] loss:2.073\n",
            "[3,   10] loss:2.078\n",
            "[3,   20] loss:2.066\n",
            "[3,   30] loss:2.072\n",
            "[3,   40] loss:2.082\n",
            "[4,   10] loss:2.068\n",
            "[4,   20] loss:2.067\n",
            "[4,   30] loss:2.083\n",
            "[4,   40] loss:2.062\n",
            "[5,   10] loss:2.068\n",
            "[5,   20] loss:2.072\n",
            "[5,   30] loss:2.066\n",
            "[5,   40] loss:2.065\n",
            "[6,   10] loss:2.067\n",
            "[6,   20] loss:2.061\n",
            "[6,   30] loss:2.055\n",
            "[6,   40] loss:2.067\n",
            "[7,   10] loss:2.062\n",
            "[7,   20] loss:2.052\n",
            "[7,   30] loss:2.056\n",
            "[7,   40] loss:2.051\n",
            "[8,   10] loss:2.047\n",
            "[8,   20] loss:2.042\n",
            "[8,   30] loss:2.049\n",
            "[8,   40] loss:2.037\n",
            "[9,   10] loss:2.040\n",
            "[9,   20] loss:2.027\n",
            "[9,   30] loss:2.024\n",
            "[9,   40] loss:2.035\n",
            "[10,   10] loss:2.029\n",
            "[10,   20] loss:2.003\n",
            "[10,   30] loss:2.011\n",
            "[10,   40] loss:2.007\n",
            "[11,   10] loss:1.986\n",
            "[11,   20] loss:1.975\n",
            "[11,   30] loss:1.983\n",
            "[11,   40] loss:2.011\n",
            "[12,   10] loss:1.999\n",
            "[12,   20] loss:2.005\n",
            "[12,   30] loss:1.950\n",
            "[12,   40] loss:1.917\n",
            "[13,   10] loss:1.982\n",
            "[13,   20] loss:1.880\n",
            "[13,   30] loss:1.902\n",
            "[13,   40] loss:1.983\n",
            "[14,   10] loss:1.938\n",
            "[14,   20] loss:1.907\n",
            "[14,   30] loss:1.865\n",
            "[14,   40] loss:1.880\n",
            "[15,   10] loss:1.867\n",
            "[15,   20] loss:1.808\n",
            "[15,   30] loss:1.915\n",
            "[15,   40] loss:1.882\n",
            "[16,   10] loss:1.793\n",
            "[16,   20] loss:1.835\n",
            "[16,   30] loss:1.823\n",
            "[16,   40] loss:1.783\n",
            "[17,   10] loss:1.719\n",
            "[17,   20] loss:1.804\n",
            "[17,   30] loss:1.755\n",
            "[17,   40] loss:1.826\n",
            "[18,   10] loss:1.684\n",
            "[18,   20] loss:1.856\n",
            "[18,   30] loss:1.764\n",
            "[18,   40] loss:1.806\n",
            "[19,   10] loss:1.766\n",
            "[19,   20] loss:1.797\n",
            "[19,   30] loss:1.621\n",
            "[19,   40] loss:1.791\n",
            "[20,   10] loss:1.678\n",
            "[20,   20] loss:1.713\n",
            "[20,   30] loss:1.736\n",
            "[20,   40] loss:1.843\n",
            "[21,   10] loss:1.688\n",
            "[21,   20] loss:1.759\n",
            "[21,   30] loss:1.709\n",
            "[21,   40] loss:1.719\n",
            "[22,   10] loss:1.643\n",
            "[22,   20] loss:1.642\n",
            "[22,   30] loss:1.684\n",
            "[22,   40] loss:1.732\n",
            "[23,   10] loss:1.729\n",
            "[23,   20] loss:1.632\n",
            "[23,   30] loss:1.639\n",
            "[23,   40] loss:1.697\n",
            "[24,   10] loss:1.642\n",
            "[24,   20] loss:1.665\n",
            "[24,   30] loss:1.690\n",
            "[24,   40] loss:1.649\n",
            "[25,   10] loss:1.587\n",
            "[25,   20] loss:1.683\n",
            "[25,   30] loss:1.718\n",
            "[25,   40] loss:1.690\n",
            "[26,   10] loss:1.706\n",
            "[26,   20] loss:1.703\n",
            "[26,   30] loss:1.722\n",
            "[26,   40] loss:1.635\n",
            "[27,   10] loss:1.704\n",
            "[27,   20] loss:1.547\n",
            "[27,   30] loss:1.845\n",
            "[27,   40] loss:1.483\n",
            "[28,   10] loss:1.541\n",
            "[28,   20] loss:1.479\n",
            "[28,   30] loss:1.717\n",
            "[28,   40] loss:1.789\n",
            "[29,   10] loss:1.716\n",
            "[29,   20] loss:1.619\n",
            "[29,   30] loss:1.504\n",
            "[29,   40] loss:1.666\n",
            "[30,   10] loss:1.591\n",
            "[30,   20] loss:1.741\n",
            "[30,   30] loss:1.582\n",
            "[30,   40] loss:1.632\n",
            "[31,   10] loss:1.603\n",
            "[31,   20] loss:1.712\n",
            "[31,   30] loss:1.564\n",
            "[31,   40] loss:1.581\n",
            "[32,   10] loss:1.560\n",
            "[32,   20] loss:1.636\n",
            "[32,   30] loss:1.560\n",
            "[32,   40] loss:1.602\n",
            "[33,   10] loss:1.553\n",
            "[33,   20] loss:1.543\n",
            "[33,   30] loss:1.643\n",
            "[33,   40] loss:1.631\n",
            "[34,   10] loss:1.542\n",
            "[34,   20] loss:1.567\n",
            "[34,   30] loss:1.562\n",
            "[34,   40] loss:1.650\n",
            "[35,   10] loss:1.483\n",
            "[35,   20] loss:1.607\n",
            "[35,   30] loss:1.606\n",
            "[35,   40] loss:1.804\n",
            "[36,   10] loss:1.517\n",
            "[36,   20] loss:1.595\n",
            "[36,   30] loss:1.696\n",
            "[36,   40] loss:1.542\n",
            "[37,   10] loss:1.583\n",
            "[37,   20] loss:1.516\n",
            "[37,   30] loss:1.764\n",
            "[37,   40] loss:1.518\n",
            "[38,   10] loss:1.595\n",
            "[38,   20] loss:1.470\n",
            "[38,   30] loss:1.721\n",
            "[38,   40] loss:1.434\n",
            "[39,   10] loss:1.624\n",
            "[39,   20] loss:1.511\n",
            "[39,   30] loss:1.545\n",
            "[39,   40] loss:1.520\n",
            "[40,   10] loss:1.630\n",
            "[40,   20] loss:1.529\n",
            "[40,   30] loss:1.399\n",
            "[40,   40] loss:1.480\n",
            "[41,   10] loss:1.515\n",
            "[41,   20] loss:1.457\n",
            "[41,   30] loss:1.645\n",
            "[41,   40] loss:1.637\n",
            "[42,   10] loss:1.526\n",
            "[42,   20] loss:1.489\n",
            "[42,   30] loss:1.500\n",
            "[42,   40] loss:1.550\n",
            "[43,   10] loss:1.685\n",
            "[43,   20] loss:1.462\n",
            "[43,   30] loss:1.454\n",
            "[43,   40] loss:1.538\n",
            "[44,   10] loss:1.484\n",
            "[44,   20] loss:1.596\n",
            "[44,   30] loss:1.584\n",
            "[44,   40] loss:1.384\n",
            "[45,   10] loss:1.568\n",
            "[45,   20] loss:1.531\n",
            "[45,   30] loss:1.489\n",
            "[45,   40] loss:1.528\n",
            "[46,   10] loss:1.473\n",
            "[46,   20] loss:1.604\n",
            "[46,   30] loss:1.567\n",
            "[46,   40] loss:1.506\n",
            "[47,   10] loss:1.531\n",
            "[47,   20] loss:1.445\n",
            "[47,   30] loss:1.497\n",
            "[47,   40] loss:1.573\n",
            "[48,   10] loss:1.409\n",
            "[48,   20] loss:1.450\n",
            "[48,   30] loss:1.496\n",
            "[48,   40] loss:1.522\n",
            "[49,   10] loss:1.362\n",
            "[49,   20] loss:1.489\n",
            "[49,   30] loss:1.430\n",
            "[49,   40] loss:1.409\n",
            "[50,   10] loss:1.507\n",
            "[50,   20] loss:1.468\n",
            "[50,   30] loss:1.392\n",
            "[50,   40] loss:1.366\n",
            "[51,   10] loss:1.435\n",
            "[51,   20] loss:1.616\n",
            "[51,   30] loss:1.461\n",
            "[51,   40] loss:1.550\n",
            "[52,   10] loss:1.347\n",
            "[52,   20] loss:1.451\n",
            "[52,   30] loss:1.529\n",
            "[52,   40] loss:1.522\n",
            "[53,   10] loss:1.480\n",
            "[53,   20] loss:1.368\n",
            "[53,   30] loss:1.432\n",
            "[53,   40] loss:1.471\n",
            "[54,   10] loss:1.399\n",
            "[54,   20] loss:1.515\n",
            "[54,   30] loss:1.458\n",
            "[54,   40] loss:1.356\n",
            "[55,   10] loss:1.399\n",
            "[55,   20] loss:1.405\n",
            "[55,   30] loss:1.451\n",
            "[55,   40] loss:1.461\n",
            "[56,   10] loss:1.346\n",
            "[56,   20] loss:1.412\n",
            "[56,   30] loss:1.555\n",
            "[56,   40] loss:1.445\n",
            "[57,   10] loss:1.551\n",
            "[57,   20] loss:1.539\n",
            "[57,   30] loss:1.483\n",
            "[57,   40] loss:1.413\n",
            "[58,   10] loss:1.439\n",
            "[58,   20] loss:1.391\n",
            "[58,   30] loss:1.317\n",
            "[58,   40] loss:1.419\n",
            "[59,   10] loss:1.635\n",
            "[59,   20] loss:1.343\n",
            "[59,   30] loss:1.215\n",
            "[59,   40] loss:1.459\n",
            "[60,   10] loss:1.350\n",
            "[60,   20] loss:1.348\n",
            "[60,   30] loss:1.453\n",
            "[60,   40] loss:1.285\n",
            "[61,   10] loss:1.299\n",
            "[61,   20] loss:1.526\n",
            "[61,   30] loss:1.262\n",
            "[61,   40] loss:1.403\n",
            "[62,   10] loss:1.253\n",
            "[62,   20] loss:1.554\n",
            "[62,   30] loss:1.318\n",
            "[62,   40] loss:1.338\n",
            "[63,   10] loss:1.285\n",
            "[63,   20] loss:1.407\n",
            "[63,   30] loss:1.263\n",
            "[63,   40] loss:1.434\n",
            "[64,   10] loss:1.374\n",
            "[64,   20] loss:1.212\n",
            "[64,   30] loss:1.450\n",
            "[64,   40] loss:1.551\n",
            "[65,   10] loss:1.350\n",
            "[65,   20] loss:1.346\n",
            "[65,   30] loss:1.396\n",
            "[65,   40] loss:1.306\n",
            "[66,   10] loss:1.250\n",
            "[66,   20] loss:1.341\n",
            "[66,   30] loss:1.293\n",
            "[66,   40] loss:1.473\n",
            "[67,   10] loss:1.338\n",
            "[67,   20] loss:1.278\n",
            "[67,   30] loss:1.336\n",
            "[67,   40] loss:1.382\n",
            "[68,   10] loss:1.337\n",
            "[68,   20] loss:1.371\n",
            "[68,   30] loss:1.322\n",
            "[68,   40] loss:1.263\n",
            "[69,   10] loss:1.324\n",
            "[69,   20] loss:1.378\n",
            "[69,   30] loss:1.372\n",
            "[69,   40] loss:1.174\n",
            "[70,   10] loss:1.359\n",
            "[70,   20] loss:1.290\n",
            "[70,   30] loss:1.229\n",
            "[70,   40] loss:1.330\n",
            "[71,   10] loss:1.212\n",
            "[71,   20] loss:1.198\n",
            "[71,   30] loss:1.406\n",
            "[71,   40] loss:1.457\n",
            "[72,   10] loss:1.267\n",
            "[72,   20] loss:1.432\n",
            "[72,   30] loss:1.273\n",
            "[72,   40] loss:1.250\n",
            "[73,   10] loss:1.283\n",
            "[73,   20] loss:1.294\n",
            "[73,   30] loss:1.247\n",
            "[73,   40] loss:1.310\n",
            "[74,   10] loss:1.218\n",
            "[74,   20] loss:1.187\n",
            "[74,   30] loss:1.462\n",
            "[74,   40] loss:1.344\n",
            "[75,   10] loss:1.340\n",
            "[75,   20] loss:1.342\n",
            "[75,   30] loss:1.253\n",
            "[75,   40] loss:1.222\n",
            "[76,   10] loss:1.215\n",
            "[76,   20] loss:1.301\n",
            "[76,   30] loss:1.329\n",
            "[76,   40] loss:1.361\n",
            "[77,   10] loss:1.346\n",
            "[77,   20] loss:1.267\n",
            "[77,   30] loss:1.453\n",
            "[77,   40] loss:1.212\n",
            "[78,   10] loss:1.235\n",
            "[78,   20] loss:1.367\n",
            "[78,   30] loss:1.238\n",
            "[78,   40] loss:1.254\n",
            "[79,   10] loss:1.157\n",
            "[79,   20] loss:1.313\n",
            "[79,   30] loss:1.173\n",
            "[79,   40] loss:1.247\n",
            "[80,   10] loss:1.375\n",
            "[80,   20] loss:1.240\n",
            "[80,   30] loss:1.359\n",
            "[80,   40] loss:1.170\n",
            "[81,   10] loss:1.421\n",
            "[81,   20] loss:1.290\n",
            "[81,   30] loss:1.162\n",
            "[81,   40] loss:1.256\n",
            "[82,   10] loss:1.148\n",
            "[82,   20] loss:1.379\n",
            "[82,   30] loss:1.184\n",
            "[82,   40] loss:1.341\n",
            "[83,   10] loss:1.216\n",
            "[83,   20] loss:1.283\n",
            "[83,   30] loss:1.280\n",
            "[83,   40] loss:1.162\n",
            "[84,   10] loss:1.205\n",
            "[84,   20] loss:1.247\n",
            "[84,   30] loss:1.320\n",
            "[84,   40] loss:1.187\n",
            "[85,   10] loss:1.146\n",
            "[85,   20] loss:1.085\n",
            "[85,   30] loss:1.368\n",
            "[85,   40] loss:1.268\n",
            "[86,   10] loss:1.171\n",
            "[86,   20] loss:1.299\n",
            "[86,   30] loss:1.220\n",
            "[86,   40] loss:1.206\n",
            "[87,   10] loss:1.238\n",
            "[87,   20] loss:1.267\n",
            "[87,   30] loss:1.246\n",
            "[87,   40] loss:1.227\n",
            "[88,   10] loss:1.221\n",
            "[88,   20] loss:1.078\n",
            "[88,   30] loss:1.124\n",
            "[88,   40] loss:1.322\n",
            "[89,   10] loss:1.195\n",
            "[89,   20] loss:1.356\n",
            "[89,   30] loss:1.239\n",
            "[89,   40] loss:1.093\n",
            "[90,   10] loss:1.098\n",
            "[90,   20] loss:1.161\n",
            "[90,   30] loss:1.136\n",
            "[90,   40] loss:1.302\n",
            "[91,   10] loss:1.317\n",
            "[91,   20] loss:1.087\n",
            "[91,   30] loss:1.186\n",
            "[91,   40] loss:1.258\n",
            "[92,   10] loss:1.080\n",
            "[92,   20] loss:1.285\n",
            "[92,   30] loss:1.277\n",
            "[92,   40] loss:1.126\n",
            "[93,   10] loss:1.048\n",
            "[93,   20] loss:1.353\n",
            "[93,   30] loss:1.128\n",
            "[93,   40] loss:1.179\n",
            "[94,   10] loss:1.276\n",
            "[94,   20] loss:1.118\n",
            "[94,   30] loss:1.098\n",
            "[94,   40] loss:1.129\n",
            "[95,   10] loss:1.014\n",
            "[95,   20] loss:1.196\n",
            "[95,   30] loss:1.247\n",
            "[95,   40] loss:1.043\n",
            "[96,   10] loss:0.989\n",
            "[96,   20] loss:1.123\n",
            "[96,   30] loss:1.254\n",
            "[96,   40] loss:1.158\n",
            "[97,   10] loss:1.109\n",
            "[97,   20] loss:1.217\n",
            "[97,   30] loss:1.320\n",
            "[97,   40] loss:1.148\n",
            "[98,   10] loss:1.102\n",
            "[98,   20] loss:1.292\n",
            "[98,   30] loss:1.050\n",
            "[98,   40] loss:1.088\n",
            "[99,   10] loss:1.211\n",
            "[99,   20] loss:1.064\n",
            "[99,   30] loss:1.245\n",
            "[99,   40] loss:1.300\n",
            "[100,   10] loss:1.244\n",
            "[100,   20] loss:1.075\n",
            "[100,   30] loss:1.048\n",
            "[100,   40] loss:1.183\n",
            "finished Traning\n"
          ]
        }
      ],
      "source": [
        "num_epoch=100\n",
        "for epoch in range(num_epoch):\n",
        "    running_loss=0.0\n",
        "    for i, data in enumerate(train_loader,0):\n",
        "        #get input and label\n",
        "        inputs,labels=data\n",
        "        #zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        #forwaed+backward+optimizer\n",
        "        outputs=model(inputs)\n",
        "        loss=criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print statistics\n",
        "        running_loss +=loss.item()\n",
        "        if i%10==9:\n",
        "            print('[%d,%5d] loss:%.3f'% (epoch+1,i+1,running_loss/10))\n",
        "            running_loss=0.0\n",
        "            \n",
        "print('finished Traning')    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the model**"
      ],
      "metadata": {
        "id": "9LElZc1PuM6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by6_peqE_bR0"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlbwW-qVgpwz"
      },
      "outputs": [],
      "source": [
        "model = torch.load('model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ldo6Qhlg59_",
        "outputId": "1754b830-2a9e-4796-c968-2d9fad6b6da5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction**"
      ],
      "metadata": {
        "id": "yHD1blWWuSV4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTUqQyx1iovF",
        "outputId": "1688973d-f7b7-4db7-b416-8b975ca6707a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6791,  4.7237, -2.1716, -1.4151, -4.4084,  3.9255, -5.7315,  4.5113],\n",
            "        [-4.0802,  3.1930, -1.7390, -1.0444, -3.9580,  2.8684,  1.2963,  3.6093]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 2.7721,  2.3247, -1.4148,  1.1009, -0.3712, -2.3335, -1.1605, -1.1644],\n",
            "        [ 0.6285,  0.3042, -2.6103,  1.4211, -1.3893, -0.8298,  3.1470,  0.7037]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-4.5460, -0.8390,  2.8055, -1.5740,  0.7924,  3.1374, -1.1882,  2.4090],\n",
            "        [-4.1059, -0.4724,  1.3100, -2.5435,  0.4394,  1.6403,  2.8034,  3.3051]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-4.5631, -0.3069,  2.3071, -2.0732,  0.3921,  2.7060, -0.0246,  2.7547],\n",
            "        [ 2.8487,  3.3348, -0.0626, -0.5760,  0.7158, -2.2565, -3.7018, -0.3968]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-3.2183,  1.2966, -0.8683,  0.5188, -3.0288,  1.6789,  0.4069,  2.8606],\n",
            "        [ 2.3970,  2.8557, -2.1502,  0.8097, -2.1860, -0.4682, -1.9218,  0.1522]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 2.5897,  2.7020, -2.0644,  1.0901, -1.7572, -1.2253, -1.5066, -0.3249],\n",
            "        [-4.9342, -2.8713,  3.2731,  5.1162, -2.7600,  2.3691, -5.3233,  2.7764]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-4.7485, -0.9769,  2.9708, -2.3825,  1.1854,  2.8878, -0.1780,  2.9089],\n",
            "        [ 2.7408,  2.8940, -1.7208,  0.7994, -1.2835, -1.4484, -1.9467, -0.4218]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-3.1753, -1.3086,  1.5141,  2.6750, -1.6740,  1.9221, -2.8104,  1.9020],\n",
            "        [-4.7515, -1.6225,  3.2227, -2.7426,  1.8716,  2.8801,  0.5556,  2.9044]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-5.0700, -2.1516,  3.9534, -2.5422,  2.3799,  3.8265, -1.0835,  2.5954],\n",
            "        [-3.3176, -0.5437,  0.6465,  2.4042, -2.4790,  1.9168, -2.1440,  2.3538]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-2.3018, -1.5340,  0.2593, -1.5655,  1.5683, -0.0067,  4.3888,  2.1583],\n",
            "        [ 2.9219,  3.1675, -0.5971, -0.0289,  0.1886, -2.2296, -2.9375, -0.5885]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 2.8679,  3.1861, -0.4874, -0.1742,  0.2001, -2.0849, -3.1438, -0.4823],\n",
            "        [ 1.5452,  1.3390, -3.7924,  2.2919, -3.0325, -0.3381,  2.0018,  0.2982]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-3.7243, -1.1213,  1.4199, -2.2894,  1.1130,  1.3014,  3.0504,  3.0233],\n",
            "        [-3.1496, -1.0525,  1.1831,  2.5134, -1.8783,  1.9091, -2.5060,  2.0039]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-4.8062, -1.1869,  3.1623, -2.3356,  1.3597,  3.0542, -0.3708,  2.6827],\n",
            "        [-4.2134, -0.3792,  1.9679, -0.9714, -0.1151,  2.7292, -0.9433,  2.4526]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 1.0301,  0.9400, -3.1735,  1.7615, -2.3698, -0.4029,  2.3395,  0.5479],\n",
            "        [-5.1990, -2.4069,  4.1879, -2.6773,  2.6759,  4.0620, -1.2078,  2.6345]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-1.6742,  4.3935, -5.0189,  0.3546, -7.6269,  6.1720, -0.4725,  4.4805],\n",
            "        [ 2.0974,  2.9123,  1.9685, -1.8776,  4.1810, -1.7362, -6.9081, -0.7289]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-4.6172, -1.2281,  3.2360, -1.6653,  1.2610,  3.3725, -1.5452,  2.2987],\n",
            "        [-3.0417, -2.7058,  1.9535, -2.0002,  2.8003,  1.4001,  2.5851,  2.1384]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 2.8988,  3.3037, -0.3837, -0.3284,  0.2704, -2.0235, -3.4524, -0.3815],\n",
            "        [-3.1736, -0.8473,  1.0210,  2.3287, -1.9251,  1.9227, -2.2940,  2.0681]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-4.9437, -1.8112,  3.7983, -2.0485,  1.9666,  3.8721, -1.7463,  2.3482],\n",
            "        [-3.9236, -1.9359,  2.2249,  3.8075, -2.3589,  2.0305, -3.8957,  2.3239]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 2.8378,  3.6160,  0.1586, -1.0374,  0.7054, -1.9271, -4.4039,  0.0264],\n",
            "        [-5.1257, -2.7399,  4.3433, -2.5119,  2.9412,  4.2743, -1.5841,  2.5156]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-3.6209, -0.5256,  0.6946, -2.2866,  0.4411,  1.0598,  3.6005,  3.2027],\n",
            "        [ 2.5091,  2.6768, -2.4101,  1.2414, -2.2004, -0.8876, -1.3001, -0.2011]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-3.3610, -0.3110,  0.1952, -2.1088,  0.1222,  0.8782,  3.8769,  3.1133],\n",
            "        [-1.4467, -1.3418, -0.4028, -0.9126,  1.1293, -0.4019,  4.2814,  1.7465]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 2.8187,  2.8745, -1.1211,  0.4602, -0.4657, -1.8966, -2.2509, -0.6455],\n",
            "        [-3.8337, -3.3345,  4.1401,  4.3232, -0.7293,  2.0386, -5.3940,  1.5075]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-3.2016, -0.9993,  1.1245,  2.5286, -1.9749,  1.9456, -2.4953,  2.0612],\n",
            "        [ 2.5942,  2.8925, -2.0421,  0.8736, -1.8307, -0.9550, -1.8485, -0.1505]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-3.4669, -1.3714,  1.9276,  1.8523, -0.9175,  2.2529, -2.7020,  1.8209],\n",
            "        [-4.3164, -0.6517,  1.8495, -2.6404,  0.7010,  2.0054,  2.1491,  3.2458]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 2.9396,  2.9876, -0.9941,  0.4314, -0.2043, -2.2044, -2.4121, -0.7671],\n",
            "        [-1.4528, -1.1511, -0.6088, -0.8013,  0.7837, -0.3596,  4.2594,  1.8833]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-4.8139, -1.5735,  3.2603, -2.7190,  1.8171,  2.9689,  0.3648,  2.8580],\n",
            "        [-5.3292, -2.6437,  4.3886, -2.5252,  2.8514,  4.3857, -1.7058,  2.5234]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[-2.7400, -1.3994,  0.6086, -1.8610,  1.4306,  0.4234,  3.9895,  2.4616],\n",
            "        [ 0.9078,  0.9169, -2.9029,  1.6043, -2.2802, -0.1916,  2.0300,  0.5159]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 2.3941,  2.5931, -2.5991,  1.3705, -2.5331, -0.6208, -1.0977, -0.0868],\n",
            "        [-2.1542, -1.1880, -0.1875, -1.4069,  1.0694, -0.1397,  4.5655,  2.2295]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for data in test_loader:\n",
        "    test_images, test_labels =data\n",
        "    predictions = model(test_images)\n",
        "    print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTPzBOcNi5yD",
        "outputId": "17b42ad9-5373-4120-e48a-fc7a33a9c320"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4rReQoai8gh",
        "outputId": "c501816e-8d91-4d5f-a013-0cd1d3339b57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.3941,  2.5931, -2.5991,  1.3705, -2.5331, -0.6208, -1.0977, -0.0868],\n",
              "        [-2.1542, -1.1880, -0.1875, -1.4069,  1.0694, -0.1397,  4.5655,  2.2295]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "predictions = model(test_images)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the network on the test data. and providing accuracy is on the way**"
      ],
      "metadata": {
        "id": "lNB89zNLuW0C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2Ks5U5IjCah",
        "outputId": "cf39b728-b5cf-4e63-8836-032cdbed3dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 50 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ejy5orqGsCrA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}